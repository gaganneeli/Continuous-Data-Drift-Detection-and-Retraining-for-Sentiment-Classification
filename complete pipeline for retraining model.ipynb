{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b82e6e8-2ab9-4ed5-8906-bac3c744bc97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained tokenizer from the 'bert_tokenizer' folder\n",
    "tokenizer = BertTokenizer.from_pretrained(r\"C:\\Users\\ngaga\\OneDrive\\Desktop\\cloudsek assignment\\1st problem\\bert_tokenizer\\bert_tokenizer\")\n",
    "\n",
    "# Load the pre-trained model from the 'bert_model' folder\n",
    "model = BertForSequenceClassification.from_pretrained(r\"C:\\Users\\ngaga\\OneDrive\\Desktop\\cloudsek assignment\\1st problem\\bert_model\\bert_model\")\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77f4e3f1-6462-411f-8e1b-3a5daf9b5e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample input text\n",
    "input_text = \"I dont like the movie\"\n",
    "\n",
    "# Tokenize input text\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits  # Get the raw output scores\n",
    "\n",
    "# Get predicted class (assuming binary classification)\n",
    "predicted_class = torch.argmax(logits, dim=1).item()\n",
    "print(f\"Predicted class: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cee11f21-181b-4dc2-87f6-8dfdb458f42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from text files has been saved to C:\\Users\\ngaga/Downloads\\review.csv\n"
     ]
    }
   ],
   "source": [
    "#FOR CREATING CSV FILES FROM TXT FOLDERS\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the directory containing the .txt files\n",
    "directory = r\"C:\\Users\\ngaga\\OneDrive\\Desktop\\cloudsek assignment\\1st problem\\PS-1\\PS-1\\IMDB\\gagan\\unsup (drift test)\"\n",
    "\n",
    "# Specify the path to your Downloads folder\n",
    "downloads_folder = os.path.expanduser('~/Downloads')  # This works for both Windows and macOS/Linux\n",
    "\n",
    "# List to store the content of each file\n",
    "data = []\n",
    "\n",
    "# Iterate over each file in the specified directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        # Open and read the content of the file\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            # Append content to the data list as a dictionary\n",
    "            data.append({'review': content})\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a CSV file in the Downloads folder\n",
    "output_csv_path = os.path.join(downloads_folder, 'review.csv')\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"Data from text files has been saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "685d8da4-f83b-4301-a760-98e2eb6c0c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter 'text' for single text input or 'csv' for batch processing from a CSV file:  csv\n",
      "Enter the path to your CSV file:  C:\\Users\\ngaga\\Downloads\\reviews.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing reviews: 100%|██████████████████████████████████████████████████████████████| 50/50 [00:09<00:00,  5.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to C:\\Users\\ngaga\\Downloads\\predicted_reviews.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#MAIN MODEL THAT CAN TAKE SINGLE OR BATCH AT A TIME\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the pre-trained tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(r\"C:\\Users\\ngaga\\OneDrive\\Desktop\\cloudsek assignment\\1st problem\\bert_tokenizer\\bert_tokenizer\")\n",
    "model = BertForSequenceClassification.from_pretrained(r\"C:\\Users\\ngaga\\OneDrive\\Desktop\\cloudsek assignment\\1st problem\\bert_model\\bert_model\")\n",
    "model.eval()\n",
    "\n",
    "# Choose input type: text or CSV\n",
    "input_type = input(\"Enter 'text' for single text input or 'csv' for batch processing from a CSV file: \").strip().lower()\n",
    "\n",
    "if input_type == 'text':\n",
    "    # Single text input\n",
    "    input_text = input(\"Enter your review: \")\n",
    "    \n",
    "    # Tokenize and make prediction\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    # Get predicted class\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    sentiment = \"Positive\" if predicted_class == 1 else \"Negative\"\n",
    "    print(f\"Predicted sentiment: {sentiment} (Class: {predicted_class})\")\n",
    "\n",
    "elif input_type == 'csv':\n",
    "    # Batch processing from CSV\n",
    "    input_csv_path = input(\"Enter the path to your CSV file: \").strip()\n",
    "    reviews_df = pd.read_csv(input_csv_path)\n",
    "    \n",
    "    # Check for either 'review' or 'reviews' column\n",
    "    review_column = 'review' if 'review' in reviews_df.columns else 'reviews' if 'reviews' in reviews_df.columns else None\n",
    "    if not review_column:\n",
    "        raise ValueError(\"The input CSV must contain either a 'review' or 'reviews' column.\")\n",
    "\n",
    "    # Initialize a list to store predictions\n",
    "    predicted_labels = []\n",
    "    \n",
    "    # Batch prediction\n",
    "    for review in tqdm(reviews_df[review_column], desc=\"Processing reviews\"):\n",
    "        inputs = tokenizer(review, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "        \n",
    "        predicted_class = torch.argmax(logits, dim=1).item()\n",
    "        predicted_labels.append(predicted_class)\n",
    "\n",
    "    # Add predictions to DataFrame and save\n",
    "    reviews_df['predicted_label'] = predicted_labels\n",
    "    reviews_df['predicted_sentiment'] = reviews_df['predicted_label'].apply(lambda x: 'Positive' if x == 1 else 'Negative')\n",
    "\n",
    "    output_csv_path = r\"C:\\Users\\ngaga\\Downloads\\predicted_reviews.csv\"\n",
    "    reviews_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Predictions saved to {output_csv_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"Invalid input. Please enter 'text' or 'csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a97f93b-a7b2-46b2-a90c-13783204189d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test reviews: 100%|█████████████████████████████████████████████████| 25000/25000 [1:54:15<00:00,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#FOR CALCULATING THE CURRENT MODEL ACCURACY \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the pre-trained tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(r\"C:\\Users\\ngaga\\OneDrive\\Desktop\\cloudsek assignment\\1st problem\\bert_tokenizer\\bert_tokenizer\")\n",
    "model = BertForSequenceClassification.from_pretrained(r\"C:\\Users\\ngaga\\OneDrive\\Desktop\\cloudsek assignment\\1st problem\\bert_model\\bert_model\")\n",
    "model.eval()\n",
    "\n",
    "# Load the test dataset\n",
    "test_csv_path = r\"C:\\Users\\ngaga\\Downloads\\reviews.csv\"  # Update this path to your test CSV file\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "\n",
    "# Check if the 'review' and 'label' columns exist\n",
    "if 'review' not in test_df.columns or 'label' not in test_df.columns:\n",
    "    raise ValueError(\"The input CSV must contain 'review' and 'label' columns.\")\n",
    "\n",
    "# Initialize a list to store predictions\n",
    "predicted_labels = []\n",
    "\n",
    "# Batch prediction\n",
    "for review in tqdm(test_df['review'], desc=\"Processing test reviews\"):\n",
    "    inputs = tokenizer(review, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    predicted_labels.append(predicted_class)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(test_df['label'], predicted_labels)\n",
    "print(f\"Model accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b3b0226-a1fc-453e-8b7b-8c6c5e28e376",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|█████████████████████████████████████████████████████████| 989/989 [03:56<00:00,  4.18it/s]\n",
      "Generating embeddings: 100%|█████████████████████████████████████████████████████████| 989/989 [04:08<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drift Score (Wasserstein Distance) between historical and new data: 0.11601508499614846\n",
      "Significant drift detected.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ngaga\\AppData\\Local\\Temp\\ipykernel_8692\\1647166517.py:37: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  historical_mean_embedding = torch.mean(torch.tensor(historical_embeddings), dim=0).numpy()\n"
     ]
    }
   ],
   "source": [
    "#CODE TO FIND OUT THE DRIFT SCORES BETWEEN TWO DATASETS\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from scipy.stats import wasserstein_distance\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # Import tqdm for the progress bar\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(r\"C:\\Users\\ngaga\\OneDrive\\Desktop\\cloudsek assignment\\1st problem\\bert_tokenizer\\bert_tokenizer\")\n",
    "model = BertModel.from_pretrained(r\"C:\\Users\\ngaga\\OneDrive\\Desktop\\cloudsek assignment\\1st problem\\bert_model\\bert_model\")\n",
    "model.eval()\n",
    "\n",
    "# Function to generate embeddings\n",
    "def generate_embeddings(text_data):\n",
    "    embeddings = []\n",
    "    # Use tqdm to create a progress bar\n",
    "    for text in tqdm(text_data, desc=\"Generating embeddings\"):\n",
    "        # Tokenize and encode\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Use the pooled output as the embedding\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "        embeddings.append(pooled_output.mean(dim=0).numpy())\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Load datasets\n",
    "historical_df = pd.read_csv(r\"C:\\Users\\ngaga\\OneDrive\\Desktop\\cloudsek assignment\\1st problem\\PS-1\\PS-1\\IMDB\\gagan\\drift (sup)\\review(sup).csv\")  # Labeled dataset\n",
    "new_df = pd.read_csv(r\"C:\\Users\\ngaga\\OneDrive\\Desktop\\cloudsek assignment\\1st problem\\PS-1\\PS-1\\IMDB\\gagan\\unsup (drift test)\\review(unsup).csv\")  # Update this path for your unlabeled dataset\n",
    "\n",
    "# Generate embeddings for historical and new data\n",
    "historical_embeddings = generate_embeddings(historical_df['review'].tolist())\n",
    "new_embeddings = generate_embeddings(new_df['review'].tolist())\n",
    "\n",
    "# Calculate mean embeddings for both datasets\n",
    "historical_mean_embedding = torch.mean(torch.tensor(historical_embeddings), dim=0).numpy()\n",
    "new_mean_embedding = torch.mean(torch.tensor(new_embeddings), dim=0).numpy()\n",
    "\n",
    "# Calculate Wasserstein Distance as drift measure\n",
    "drift_score = wasserstein_distance(\n",
    "    historical_mean_embedding,\n",
    "    new_mean_embedding\n",
    ")\n",
    "\n",
    "print(f\"Drift Score (Wasserstein Distance) between historical and new data: {drift_score}\")\n",
    "\n",
    "# Optional: Threshold for determining significant drift\n",
    "threshold = 0.1  # Set a threshold value based on domain knowledge or experimentation\n",
    "if drift_score > threshold:\n",
    "    print(\"Significant drift detected.\")\n",
    "else:\n",
    "    print(\"No significant drift detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "adb3cc37-51c4-43b6-aa7a-790f9c80d943",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting labels and confidence scores: 100%|███████████████████████████████████████| 989/989 [03:53<00:00,  4.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered predictions with confidence scores >= 0.8 saved to C:\\Users\\ngaga\\Downloads\\filtered_predicted_reviews_with_confidence.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#PREDICTING OF UNLABELED DATA AND FILTERING BASED ON CONFIDENCE VALUE OF 0.8.\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "# Load the pre-trained tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(r\"C:\\Users\\ngaga\\OneDrive\\Desktop\\cloudsek assignment\\1st problem\\bert_tokenizer\\bert_tokenizer\")\n",
    "model = BertForSequenceClassification.from_pretrained(r\"C:\\Users\\ngaga\\OneDrive\\Desktop\\cloudsek assignment\\1st problem\\bert_model\\bert_model\")\n",
    "model.eval()\n",
    "\n",
    "# Load the unlabeled dataset\n",
    "unlabeled_df = pd.read_csv(r\"C:\\Users\\ngaga\\OneDrive\\Desktop\\cloudsek assignment\\1st problem\\PS-1\\PS-1\\IMDB\\gagan\\unsup (drift test)\\review(unsup).csv\")\n",
    "\n",
    "# Lists to store the filtered review, predicted label, and confidence score\n",
    "filtered_reviews = []\n",
    "filtered_labels = []\n",
    "filtered_confidence_scores = []\n",
    "\n",
    "# Prediction function for each review\n",
    "def predict_label_and_confidence(review_text):\n",
    "    inputs = tokenizer(review_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get logits and apply softmax to obtain probabilities\n",
    "    logits = outputs.logits\n",
    "    probabilities = softmax(logits, dim=1)\n",
    "    \n",
    "    # Predicted label and confidence score\n",
    "    predicted_label = torch.argmax(probabilities, dim=1).item()\n",
    "    confidence_score = torch.max(probabilities).item()\n",
    "    \n",
    "    return predicted_label, confidence_score\n",
    "\n",
    "# Iterate over each review in the unlabeled dataset with a progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "for review in tqdm(unlabeled_df['review'], desc=\"Predicting labels and confidence scores\"):\n",
    "    label, confidence = predict_label_and_confidence(review)\n",
    "    # Only store predictions with confidence >= 0.8\n",
    "    if confidence >= 0.8:\n",
    "        filtered_reviews.append(review)\n",
    "        filtered_labels.append(label)\n",
    "        filtered_confidence_scores.append(confidence)\n",
    "\n",
    "# Create a DataFrame with filtered data\n",
    "filtered_df = pd.DataFrame({\n",
    "    'review': filtered_reviews,\n",
    "    'predicted_label': filtered_labels,\n",
    "    'confidence_score': filtered_confidence_scores\n",
    "})\n",
    "\n",
    "# Save to CSV in the Downloads folder\n",
    "output_csv_path = r\"C:\\Users\\ngaga\\Downloads\\filtered_predicted_reviews_with_confidence.csv\"\n",
    "filtered_df.to_csv(output_csv_path, index=False)\n",
    "print(f\"Filtered predictions with confidence scores >= 0.8 saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6c0a6a0-3d8e-4695-a95e-a7d1d00e0880",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PREPROCESSING FOR RETRAINING OF MODEL\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Load the filtered dataset\n",
    "filtered_df = pd.read_csv(r\"C:\\Users\\ngaga\\OneDrive\\Desktop\\cloudsek assignment\\1st problem\\PS-1\\PS-1\\IMDB\\gagan\\filtered_predicted_reviews_with_confidence.csv\")\n",
    "\n",
    "# Define a custom dataset\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, reviews, labels, tokenizer, max_length=128):\n",
    "        self.reviews = reviews\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        review = self.reviews[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        # Tokenize and encode\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'review_text': review,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Prepare data for training\n",
    "X = filtered_df['review'].tolist()\n",
    "y = filtered_df['predicted_label'].tolist()\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(r\"C:\\Users\\ngaga\\OneDrive\\Desktop\\cloudsek assignment\\1st problem\\bert_tokenizer\\bert_tokenizer\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ReviewDataset(X_train, y_train, tokenizer)\n",
    "val_dataset = ReviewDataset(X_val, y_val, tokenizer)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1123797-3350-4c2a-a830-c90462b30cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 46/46 [03:41<00:00,  4.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.1250\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 46/46 [03:54<00:00,  5.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.0352\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 46/46 [03:52<00:00,  5.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.0116\n",
      "Model retraining complete and saved.\n"
     ]
    }
   ],
   "source": [
    "#CODE FOR RETRAINING THE MODEL\n",
    "\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the pre-trained model and set it to CPU\n",
    "model = BertForSequenceClassification.from_pretrained(r\"C:\\Users\\ngaga\\OneDrive\\Desktop\\cloudsek assignment\\1st problem\\bert_model\\bert_model\")\n",
    "model.train()\n",
    "\n",
    "# Set up the optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "total_steps = len(train_loader) * 3  # 3 epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3):  # Train for 3 epochs\n",
    "    print(f\"Epoch {epoch + 1}/{3}\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Move data to CPU\n",
    "        input_ids = batch['input_ids']  # No need to move to GPU\n",
    "        attention_mask = batch['attention_mask']  # No need to move to GPU\n",
    "        labels = batch['labels']  # No need to move to GPU\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Average Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Save the retrained model\n",
    "model.save_pretrained(r\"C:\\Users\\ngaga\\OneDrive\\Desktop\\cloudsek assignment\\1st problem\\bert_model\\bert_model_retrained\")\n",
    "tokenizer.save_pretrained(r\"C:\\Users\\ngaga\\OneDrive\\Desktop\\cloudsek assignment\\1st problem\\bert_tokenizer\\bert_tokenizer_retrained\")\n",
    "\n",
    "print(\"Model retraining complete and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab56cffb-0ac3-487e-a7bd-a5273f673c38",
   "metadata": {},
   "source": [
    "## Complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9864394-0010-48ea-b0bb-751af0c3b875",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################## MAIN CODE ###########################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import wasserstein_distance\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the pre-trained tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(r\"C:\\Users\\ngaga\\OneDrive\\Desktop\\cloudsek assignment\\1st problem\\bert_tokenizer\\bert_tokenizer\")\n",
    "model = BertForSequenceClassification.from_pretrained(r\"C:\\Users\\ngaga\\OneDrive\\Desktop\\cloudsek assignment\\1st problem\\bert_model\\bert_model\")\n",
    "model.eval()\n",
    "\n",
    "# Function to make predictions\n",
    "def predict_sentiment(input_data):\n",
    "    inputs = tokenizer(input_data, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    return predicted_class\n",
    "\n",
    "# Choose input type: text or CSV\n",
    "input_type = input(\"Enter 'text' for single text input or 'csv' for batch processing from a CSV file: \").strip().lower()\n",
    "\n",
    "if input_type == 'text':\n",
    "    # Single text input\n",
    "    input_text = input(\"Enter your review: \")\n",
    "    predicted_class = predict_sentiment(input_text)\n",
    "    sentiment = \"Positive\" if predicted_class == 1 else \"Negative\"\n",
    "    print(f\"Predicted sentiment: {sentiment} (Class: {predicted_class})\")\n",
    "\n",
    "    # Save the review to a CSV file in append mode\n",
    "    review_records = [input_text]\n",
    "    output_review_csv_path = r\"C:\\Users\\ngaga\\Downloads\\review_predictions.csv\"\n",
    "    review_df = pd.DataFrame(review_records, columns=['review'])\n",
    "    review_df.to_csv(output_review_csv_path, mode='a', index=False, header=not pd.io.common.file_exists(output_review_csv_path))\n",
    "    print(f\"Review saved to {output_review_csv_path}.\")\n",
    "\n",
    "elif input_type == 'csv':\n",
    "    # Batch processing from CSV\n",
    "    input_csv_path = input(\"Enter the path to your CSV file: \").strip()\n",
    "    reviews_df = pd.read_csv(input_csv_path)\n",
    "\n",
    "    # Check for either 'review' or 'reviews' column\n",
    "    review_column = 'review' if 'review' in reviews_df.columns else 'reviews' if 'reviews' in reviews_df.columns else None\n",
    "    if not review_column:\n",
    "        raise ValueError(\"The input CSV must contain either a 'review' or 'reviews' column.\")\n",
    "\n",
    "    # Initialize a list to store predictions\n",
    "    predicted_labels = []\n",
    "    review_records = []  # To store the reviews for CSV saving\n",
    "\n",
    "    # Batch prediction\n",
    "    for review in tqdm(reviews_df[review_column], desc=\"Processing reviews\"):\n",
    "        predicted_class = predict_sentiment(review)\n",
    "        predicted_labels.append(predicted_class)\n",
    "        review_records.append(review)  # Collect the review for saving\n",
    "\n",
    "    # Add predictions to DataFrame and save\n",
    "    reviews_df['predicted_label'] = predicted_labels\n",
    "    reviews_df['predicted_sentiment'] = reviews_df['predicted_label'].apply(lambda x: 'Positive' if x == 1 else 'Negative')\n",
    "\n",
    "    output_csv_path = r\"C:\\Users\\ngaga\\Downloads\\predicted_reviews.csv\"\n",
    "    reviews_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Predictions saved to {output_csv_path}.\")\n",
    "\n",
    "    # Save only the reviews to a CSV file in append mode\n",
    "    output_review_csv_path = r\"C:\\Users\\ngaga\\Downloads\\review_predictions.csv\"\n",
    "    review_df = pd.DataFrame(review_records, columns=['review'])\n",
    "    review_df.to_csv(output_review_csv_path, mode='a', index=False, header=not pd.io.common.file_exists(output_review_csv_path))\n",
    "    print(f\"Reviews saved to {output_review_csv_path}.\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "# Function to safely parse embeddings from strings\n",
    "def parse_embedding(embedding_str):\n",
    "    # Ensure the string is treated as a list of floats\n",
    "    return list(map(float, embedding_str.strip(\"[]\").split()))\n",
    "\n",
    "# Function to generate embeddings\n",
    "def generate_embeddings(reviews):\n",
    "    embeddings = []\n",
    "    for review in tqdm(reviews, desc=\"Generating embeddings\"):\n",
    "        inputs = tokenizer(review, return_tensors='pt', truncation=True, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            embedding = logits.numpy().flatten()  # Flatten logits as embedding\n",
    "        embeddings.append(embedding)\n",
    "    return embeddings\n",
    "\n",
    "# Load reviews from 'csv_reviews.csv'\n",
    "input_csv_path = r\"C:\\Users\\ngaga\\Downloads\\review_predictions.csv\"\n",
    "review_df = pd.read_csv(input_csv_path)\n",
    "\n",
    "# Check if there are at least 900 reviews\n",
    "if len(review_df) >= 900:\n",
    "    print(\"Generating embeddings and starting data drift calculation...\")\n",
    "\n",
    "    # Generate embeddings for reviews\n",
    "    review_embeddings = generate_embeddings(review_df['review'].tolist())\n",
    "    review_df['embedding'] = review_embeddings\n",
    "\n",
    "    # Save updated 'csv_reviews.csv' with embeddings\n",
    "    review_df.to_csv(input_csv_path, index=False)\n",
    "    print(f\"Embeddings added to existing file: {input_csv_path}\")\n",
    "\n",
    "    # Load historical embeddings\n",
    "    historical_embeddings_csv_path = r\"C:\\Users\\ngaga\\Downloads\\embeddings.csv\"\n",
    "    historical_embeddings_df = pd.read_csv(historical_embeddings_csv_path)\n",
    "\n",
    "    # Parse stored embeddings from strings\n",
    "    historical_embeddings = historical_embeddings_df['embedding'].apply(parse_embedding).tolist()\n",
    "    current_embeddings = review_df['embedding'].tolist()\n",
    "\n",
    "    # Calculate mean embeddings\n",
    "    historical_mean_embedding = torch.mean(torch.tensor(historical_embeddings), dim=0).numpy()\n",
    "    current_mean_embedding = torch.mean(torch.tensor(current_embeddings), dim=0).numpy()\n",
    "\n",
    "    # Calculate Wasserstein Distance for drift detection\n",
    "    drift_score = wasserstein_distance(historical_mean_embedding, current_mean_embedding)\n",
    "    print(f\"Drift Score: {drift_score}\")\n",
    "\n",
    "    # Optional: Threshold for drift detection\n",
    "    threshold = 1.0\n",
    "    if drift_score > threshold:\n",
    "        print(\"Significant drift detected. Generating predictions for reviews.\")\n",
    "\n",
    "        # Initialize lists for predictions and confidence scores\n",
    "        predicted_labels = []\n",
    "        confidence_scores = []\n",
    "\n",
    "        # Predict labels and confidence scores\n",
    "        for review in tqdm(review_df['review'], desc=\"Predicting labels\"):\n",
    "            inputs = tokenizer(review, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                logits = outputs.logits\n",
    "                probabilities = torch.softmax(logits, dim=1)\n",
    "                confidence, predicted_class = torch.max(probabilities, dim=1)\n",
    "                predicted_labels.append(predicted_class.item())\n",
    "                confidence_scores.append(confidence.item())\n",
    "\n",
    "        # Add labels and confidence scores to DataFrame\n",
    "        review_df['predicted_label'] = predicted_labels\n",
    "        review_df['confidence_score'] = confidence_scores\n",
    "\n",
    "        # Filter by confidence score >= 0.8\n",
    "        review_df = review_df[review_df['confidence_score'] >= 0.8]\n",
    "\n",
    "        # Save the updated CSV\n",
    "        review_df.to_csv(input_csv_path, index=False)\n",
    "        print(f\"Updated review predictions saved to {input_csv_path}.\")\n",
    "\n",
    "       # DATA PREPROCESSING FOR RETRAINING OF MODEL\n",
    "\n",
    "        import torch\n",
    "        from transformers import BertTokenizer, BertForSequenceClassification\n",
    "        from torch.utils.data import DataLoader, Dataset\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        import pandas as pd\n",
    "        \n",
    "        # Load the filtered dataset\n",
    "        filtered_df = pd.read_csv(r\"C:\\Users\\ngaga\\Downloads\\review_predictions.csv\")\n",
    "        \n",
    "        # Define a custom dataset\n",
    "        class ReviewDataset(Dataset):\n",
    "            def __init__(self, reviews, labels, tokenizer, max_length=128):\n",
    "                self.reviews = reviews\n",
    "                self.labels = labels\n",
    "                self.tokenizer = tokenizer\n",
    "                self.max_length = max_length\n",
    "        \n",
    "            def __len__(self):\n",
    "                return len(self.reviews)\n",
    "        \n",
    "            def __getitem__(self, index):\n",
    "                review = self.reviews[index]\n",
    "                label = self.labels[index]\n",
    "        \n",
    "                # Tokenize and encode\n",
    "                encoding = self.tokenizer.encode_plus(\n",
    "                    review,\n",
    "                    add_special_tokens=True,\n",
    "                    max_length=self.max_length,\n",
    "                    return_token_type_ids=False,\n",
    "                    padding='max_length',\n",
    "                    truncation=True,\n",
    "                    return_attention_mask=True,\n",
    "                    return_tensors='pt',\n",
    "                )\n",
    "        \n",
    "                return {\n",
    "                    'review_text': review,\n",
    "                    'input_ids': encoding['input_ids'].flatten(),\n",
    "                    'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                    'labels': torch.tensor(label, dtype=torch.long)\n",
    "                }\n",
    "        \n",
    "        # Prepare data for training\n",
    "        X = filtered_df['review'].tolist()\n",
    "        y = filtered_df['predicted_label'].tolist()\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Initialize the tokenizer\n",
    "        tokenizer = BertTokenizer.from_pretrained(r\"C:\\Users\\ngaga\\OneDrive\\Desktop\\cloudsek assignment\\1st problem\\bert_tokenizer\\bert_tokenizer\")\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = ReviewDataset(X_train, y_train, tokenizer)\n",
    "        val_dataset = ReviewDataset(X_val, y_val, tokenizer)\n",
    "        \n",
    "        # Create DataLoaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "        \n",
    "        \n",
    "        # CODE FOR RETRAINING THE MODEL\n",
    "        \n",
    "        from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "        from tqdm import tqdm\n",
    "        \n",
    "        # Set device to GPU if available\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Load the pre-trained model and set it to the device\n",
    "        model = BertForSequenceClassification.from_pretrained(r\"C:\\Users\\ngaga\\OneDrive\\Desktop\\cloudsek assignment\\1st problem\\bert_model\\bert_model\")\n",
    "        model.to(device)\n",
    "        model.train()\n",
    "        \n",
    "        # Set up the optimizer and scheduler\n",
    "        optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "        total_steps = len(train_loader) * 3  # 3 epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(3):  # Train for 3 epochs\n",
    "            print(f\"Epoch {epoch + 1}/{3}\")\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "        \n",
    "            for batch in tqdm(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Move data to device\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                total_loss += loss.item()\n",
    "        \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "        \n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            print(f\"Average Training Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Save the retrained model, replacing the existing model\n",
    "        model.save_pretrained(r\"C:\\Users\\ngaga\\OneDrive\\Desktop\\cloudsek assignment\\1st problem\\bert_model\\bert_model\")\n",
    "        tokenizer.save_pretrained(r\"C:\\Users\\ngaga\\OneDrive\\Desktop\\cloudsek assignment\\1st problem\\bert_tokenizer\\bert_tokenizer\")\n",
    "        \n",
    "        print(\"Model retraining complete and saved.\")\n",
    "    else:\n",
    "        print(\"No significant drift detected. No predictions needed.\")\n",
    "else:\n",
    "    print(f\"Number of reviews: {len(review_df)}. Waiting for at least 900 reviews.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
